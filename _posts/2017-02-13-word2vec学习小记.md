---
title: word2vec学习小记
date: 2017-02-13 14:39:39
tags: [word2vec, NLP, Google, 词向量, 入门, 教程, 实例, 自然语言处理]
categories: NLP
link_title: learning_word2vec
---
word2vec是Google于2013年开源推出的一个用于获取词向量的工具包，它简单、高效，因此引起了很多人的关注。最近项目组使用word2vec来实现个性化搜索，在阅读资料的过程中做了一些笔记，用于后面进一步学习。
<!-- more -->

**前注：word2vec涉及的相关理论和推导是非常严(ku)格(zao)的，本文作为一个初学者的学习笔记，希望能从自己的理解中尽量用简单的描述，如有错误或者歧义的地方，欢迎指正。**

## word2vec是什么？

> This tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. These representations can be subsequently used in many natural language processing applications and for further research.

从官方的介绍可以看出word2vec是一个将词表示为一个向量的工具，通过该向量表示，可以用来进行更深入的自然语言处理，比如机器翻译等。

为了理解word2vec的设计思想，我们有必要先学习一下自然语言处理的相关发展历程和基础知识。

## 基础知识
### 语言模型
- 语言模型其实就是看一句话是不是正常人说出来的。这玩意很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在 NLP 的其它任务里也都能用到。
- 用数学表达的话，就是给定T个词**w1,w2,...wT**，看它是自然语言的概率P，公式如下所示：

![语言模型形式化表达](language_model.png)

- 举例来说，比如一段语音识别为“我喜欢吃梨”和“我喜欢吃力”，根据分词和上述公司，可以得到两个表述的概率计算分别为下面的公司，而其中每一个子项的概率我们可以事先通过大量的语料统计得到，这样我们就可以得到更合理的结果。


    P('我喜欢吃梨') = P('我') * P('喜欢'|'我') * P('吃'|'我','喜欢') * P('梨'|'我','喜欢','吃')
    P('我喜欢吃力') = P('我') * P('喜欢'|'我') * P('吃力'|'我','喜欢')

### N-gram模型
- 通过上面的语言模型计算的例子，大家可以发现，如果一个句子比较长，那么它的计算量会很大；
- 牛逼的科学家们想出了一个**N-gram模型**来简化计算，在计算某一项的概率时Context不是考虑前面所有的词，而是前**N-1**个词；
- 当然牛逼的科学家们还在此模型上继续优化，比如**N-pos模型**从语法的角度出发，先对词进行词性标注分类，在此基础上来计算模型的概率；后面还有一些针对性的语言模型改进，这里就不一一介绍。
- 通过上面简短的语言模型介绍，我们可以看出核心的计算在于P(w|Content)，对于其的计算主要有两种思路：一种是基于统计的思路，另外一种是通过函数拟合的思路；前者比较容易理解但是实际运用的时候有一些问题（比如如果组合在语料里没出现导致对应的条件概率变为0等），而函数拟合的思路就是通过语料的输入训练出一个函数用于表达P(w|Content)，这样对于测试数据就直接套用函数计算概率即可，这也是机器学习中惯用的思路之一。

### 词向量表示
- 自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。
- 最直观的就是把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为0，只有一个维度的值为1，这个维度就代表了当前的词。这种表示方式被称为**One-hot Representation**。这种方式的优点在于简洁，但是却无法描述词与词之间的关系。
- 另外一种表示方法是通过一个低维的向量（通常为50维、100维或200维），其基于“**具有相似上下文的词，应该具有相似的语义**”的假说，这种表示方式被称为**Distributed Representation**。它是一个稠密、低维的实数向量，它的每一维表示词语的一个潜在特征，该特征捕获了有用的句法和语义特征。其特点是将词语的不同句法和语义特征分布到它的每一个维度上去表示。这种方式的好处是可以通过空间距离或者余弦夹角来描述词与词之间的相似性。
- 一个词的向量表示对于不同的语料和场景结果是不同的，下面就介绍一种最经典的计算词向量的语言模型。

### 神经网络概率语言模型
- 神经网络概率语言模型（NNLM）把词向量作为输入（初始的词向量是随机值），训练语言模型的同时也在训练词向量，最终可以同时得到语言模型和词向量。
- Bengio等牛逼的科学家们用了一个三层的神经网络来构建语言模型，同样也是N-gram 模型。 网络的第一层是输入层，是是上下文的N-1个向量组成的(n-1)m维向量；第二层是隐藏层，使用tanh作为激活函数；第三层是输出层，每个节点表示一个词的未归一化概率，最后使用softmax激活函数将输出值归一化。

![神经网络概率语言模型](nnlm.png)

- 得到这个模型，然后就可以利用梯度下降法把模型优化出来，最终得到语言模型和词向量表示。

### word2vec的核心模型
word2vec在NNLM的基础上进行了优化，有CBOW模型和Skip-Gram模型，还有Hierarchical Softmax和Negative Sampling两套框架，两两组合出四套算法模型。


## 通过实战了解word2vec
### 下载语料
从搜狗实验室下载[全网新闻数据(SogouCA)](http://www.sogou.com/labs/resource/ca.php)，该语料来自若干新闻站点2012年6月—7月期间国内、国际、体育、社会、娱乐等18个频道的新闻数据。

下载该文件解压后大约为1.5G，包含120w条以上的新闻，文件的内容格式如下图所示：

![搜狗CA语料内容格式](http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/corpus_format.png)

### 获取新闻内容
这里我们先对语料进行初步处理，只获取新闻内容部分。可以执行以下命令获取content部分：

    cat news_tensite_xml.dat | iconv -f gbk -t utf-8 -c | grep "<content>"  > corpus.txt

### 分词处理
由于word2vec处理的数据是单词分隔的语句，对于中文来说，需要先进行分词处理。这里采用的是中国自然语言处理开源组织开源的[ansj_seg](https://github.com/NLPchina/ansj_seg)分词器，核心代码如下所示：

```java
public class WordAnalyzer {
    private static final String TAG_START_CONTENT = "<content>";
    private static final String TAG_END_CONTENT = "</content>";
    private static final String INPUT_FILE = "/home/test/w2v/corpus.txt";
    private static final String OUTPUT_FILE = "/home/test/w2v/corpus_out.txt";

    public static void main(String[] args) throws Exception {
        BufferedReader reader = null;
        PrintWriter pw = null;
        try {
            System.out.println("开始处理分词...");
            reader = IOUtil.getReader(INPUT_FILE, "UTF-8");
            pw = new PrintWriter(OUTPUT_FILE);
            long start = System.currentTimeMillis();
            int totalCharactorLength = 0;
            int totalTermCount = 0;
            Set<String> set = new HashSet<String>();
            String temp = null;
            while ((temp = reader.readLine()) != null) {
                temp = temp.trim();
                if (temp.startsWith(TAG_START_CONTENT)) {
                    //System.out.println("处理文本:" + temp);
                    int end = temp.indexOf(TAG_END_CONTENT);
                    String content = temp.substring(TAG_START_CONTENT.length(), end);
                    totalCharactorLength += content.length();
                    Result result = ToAnalysis.parse(content);
                    for (Term term : result) {
                        String item = term.getName().trim();
                        totalTermCount++;
                        pw.print(item + " ");
                        set.add(item);
                    }
                    pw.println();
                }
            }

            long end = System.currentTimeMillis();
            System.out.println("共" + totalTermCount + "个Term，共" 
                + set.size() + "个不同的Term，共 " 
                + totalCharactorLength + "个字符，每秒处理字符数:" 
                + (totalCharactorLength * 1000.0 / (end - start)));
        } finally {
            // close reader and pw
        }
    }
}
```

分词处理之后的文件内容如下所示：
![分词之后的语料](http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/corpus_ansj_analyze.png)

### 下载word2vec源码并编译
这里我没有从官网下载而是从github上的[svn2github/word2vec](https://github.com/svn2github/word2vec)项目下载源码，下载之后执行make命令编译，这个过程很快就可以结束。

### 开始word2vec处理
编译成功后开始处理。我这里用的是CentOS 64位的虚拟机，八核CPU，32G内存，整个处理过程耗时大约4个小时。

    ./word2vec -train ../corpus_out.txt -output vectors.bin -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1

### 测试处理结果
处理结束之后，使用distance命令可以测试处理结果，以下是分别测试【足球】和【改革】的效果：

![测试足球一词的词向量](http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/w2v_test_football.png)

![测试改革一词的词向量](http://oi46mo3on.bkt.clouddn.com/14_learning_w2v/w2v_test_reform.png)


## 学习小结
- word2vec的模型是基于神经网络来训练词向量的工具；
- word2vec通过一系列的模型和框架对原有的NNLM进行优化，简化了计算但准确度还是保持得很好；

## 参考材料
- [word2vec HomePage](https://code.google.com/archive/p/word2vec/)
- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
- [Deep Learning in NLP （一）词向量和语言模型](http://licstar.net/archives/328)
- [word2vec 中的数学原理详解](http://blog.csdn.net/itplus/article/details/37969635)
- [利用word2vec对关键词进行聚类](http://blog.csdn.net/zhaoxinfan/article/details/11069485)
- [word2vec词向量训练及中文文本相似度计算](http://blog.csdn.net/eastmount/article/details/50637476)
- [词表示模型（一）：表示学习；syntagmatic与paradigmatic两类模型；基于矩阵的LSA和GloVe](http://www.cnblogs.com/Determined22/p/5780305.html)
- [词表示模型（二）：基于神经网络的模型：NPLM；word2vec（CBOW/Skip-gram）](http://www.cnblogs.com/Determined22/p/5804455.html)
- [词表示模型（三）：word2vec（CBOW/Skip-gram）的加速：Hierarchical Softmax与Negative Sampling](http://www.cnblogs.com/Determined22/p/5807362.html)
- [深度学习word2vec笔记之基础篇](http://blog.csdn.net/mytestmy/article/details/26961315)
- [Google 开源项目 word2vec 的分析？_杨超的回答](https://www.zhihu.com/question/21661274/answer/19331979)
- [Word2Vec-知其然知其所以然](https://www.zybuluo.com/Dounm/note/591752)
- [word2vec 原理篇](http://www.nustm.cn/blog/index.php/archives/842)